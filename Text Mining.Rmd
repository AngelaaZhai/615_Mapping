---
title: "TextMining HW"
author: "Kaiyu Yan, Fionnuala McPeake, Angela Zhai, Miller Xu"
date: "November 2, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE, message=FALSE}
library(tidytext)
library(readr)
library(tidytext)
library(tidyverse)
library(tidyr)
library(scales)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
library(topicmodels)
```

##  Introduction  
In order to better understand the functions of tidytext, we have selected two blog posts to compare. The first article, "Reconstructing Cambridge Analytica's "Psychological Warfare Tool"" discusses how Cambridge Analytic used people's Facebook profiles in order microtarget of ads to the Trump presidential campaign, gives a simplified example of how such a thing can be done in R, and discusses the results and how they effective Cambridge Analytic targeted ads given what we have learned. The second article, "About p-values", discusses common misconseptions about p-values. These articles were selected becayse they both are of a substatnial length for a blog post, and take a stance on a topic. 

## Chapter 1
Plots were created to count the occurences of frequently used words in both articles. The biggest suprise for the second article is that "p-value" was not on the list, but because of the hyphen and the common use of "value" and "values", it's count was diluted. The graph showing the corrolation of common words between the two articles show that they are not particularly comparable to eachother, with an R value of 0.45. This is not suprising, as one is much more focused on statistical theory than the other.   

```{r echo = FALSE, message = FALSE,warning=FALSE}
### First Article
Web1 <- read.delim2("Article1.txt")
# break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure
Web1 <- data.frame(lapply(Web1, as.character), stringsAsFactors = FALSE) %>%
  mutate(linenumber = row_number())
colnames(Web1) <- c("text", "linenumber")
Web1 %>% unnest_tokens(output = word, input = text) -> Web1
# remove stop words
data("stop_words")
Web1 <- Web1 %>%
  anti_join(stop_words)
# create a visualization of the most common words
Web1 %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()+
  ggtitle("Article 1") +
  theme(plot.title = element_text(hjust = 0.5))

### Second Article
Web2 <- read.delim2("Article2.txt")
# break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure
Web2 <- data.frame(lapply(Web2, as.character), stringsAsFactors = FALSE) %>%
  mutate(linenumber = row_number())
colnames(Web2) <- c("text", "linenumber")
Web2 %>% unnest_tokens(output = word, input = text) -> Web2
# remove stop words
Web2 <- Web2 %>%
  anti_join(stop_words)
# create a visualization of the most common words
Web2 %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  ylab("Count") +
  coord_flip()+
  ggtitle("Article 2") +
  theme(plot.title = element_text(hjust = 0.5))

### Word Frequency
# calculate the frequency for each word of two Articles
frequency <- bind_rows(
  mutate(Web1, Webpage = "Article1"),
  mutate(Web2, Webpage = "Article2")
) %>%
  count(Webpage, word) %>%
  group_by(Webpage) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(Webpage, proportion) %>%
  gather(Webpage, proportion, Article1)
# Comparing the word frequencies of two Articles
ggplot(frequency, aes(x = proportion, y = Article2, color = abs(Article2 - proportion))) +
  geom_abline(color = "black", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "red", high = "blue") +
  facet_wrap(~Webpage, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Article2", x = NULL)
# quantify how similar and different these sets of word frequencies
cor.test(
  data = frequency[frequency$Webpage == "Article1", ],
  ~proportion + `Article2`
)
```

##Chapter 2

A sentiment analyis was preformed on each article, indicating the progression in terms of positivity and negativity. Article 1 was longer than Article 2, and less negative, as indicated by the x- and y-axis, respectively. The second article seems to be more negative, not only because it's y-axis spans -5 to 5, as opposed to +/-4 for the first article, but in the frequency of negative bars. The second article was instructional, and did cast doubt on commonly held thoughts on p-value interpritation, but was overall was not opinionated. This negativity is likely due to the commonly word used "mean", here of course refering to average as opposed to being used as an adjective. This is a perfect illustration of the limitations of text analysis misinterpreting words and not being able to take context into account. 

```{r echo = FALSE, message = FALSE,warning=FALSE}
nrc_joy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

Web1 %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
Web2 %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

Article_1_sentiment <- Web1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

Article_2_sentiment <- Web2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(Article_1_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle("Article1")+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(Article_2_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle("Article2")+
  theme(plot.title = element_text(hjust = 0.5))
  
```

The analysis above was re-preformed comparing all of the sentiment lexicons. Again, the second article is shown to be shorter than the first article. However, the second article seems to be more positive in these results than when the net sentiment was calculated. As in the original analysi, the first article has longer stretches where no sentiment is recorded. This is likely because it contains instructions and code, which cannot be easily interprited in terms of emotion. 

```{r echo = FALSE, message=FALSE,warning=FALSE}
### Article 1
# calculate the sentiment in different ways
afinn <- Web1 %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber) %>%
  summarise(sentiment = sum(score)) %>%
  mutate(method = "AFINN")
# calculate the sentiment in different ways
bing_and_nrc <- bind_rows(
  Web1 %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  Web1 %>%
    inner_join(get_sentiments("nrc") %>%
      filter(sentiment %in% c(
        "positive",
        "negative"
      ))) %>%
    mutate(method = "NRC")
) %>%
  count(method, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
# bind them together and visualize
bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")+
   ggtitle("Article 1")+
  theme(plot.title = element_text(hjust = 0.5))

### Article 2
# calculate the sentiment in different ways
afinn <- Web2 %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(index = linenumber) %>%
  summarise(sentiment = sum(score)) %>%
  mutate(method = "AFINN")
# calculate the sentiment in different ways
bing_and_nrc <- bind_rows(
  Web2 %>%
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  Web2 %>%
    inner_join(get_sentiments("nrc") %>%
      filter(sentiment %in% c(
        "positive",
        "negative"
      ))) %>%
    mutate(method = "NRC")
) %>%
  count(method, index = linenumber, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
# bind them together and visualize
bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")+
    ggtitle("Article 2")+
  theme(plot.title = element_text(hjust = 0.5))
```

The most frequent words of each sentiment was presented in order of frequency in the graphs below. While some of the previous analyses have indicated that the second article is overall negative, there are more words associated with positive sentiment than negative, and one particular word, "significant", makes up the majority of the counts. As this was an instructional statistical article, the word "significant" here should take on a neutral connotation. This may explain why some lexicons rated the second article more positive than when it was not broken down into specific indexes. 

The results for the first article contains many more words than the second article, likely because it was more opinionated and not constrained by the need to use commonly known phrases related to it's topic. There seems to be an even balance of negative and positive words, and all seem to be appropriately categorized, with one exception. The word "Trump" here refers to the name of a person, not the verb. While many people have strong opinions on Donald Trump, it should be treated neutrally in this analysis. 

```{r echo = FALSE, message=FALSE,warning=FALSE}
#Article 1
#find out how much each word contributed to each sentiment
bing_word_counts_1<- Web1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts_1 %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment, Article 1",
       x = NULL) +
  coord_flip()

#Article 2
#find out how much each word contributed to each sentiment
bing_word_counts_2<- Web2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts_2 %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment, Article 2",
       x = NULL) +
  coord_flip()
```

Word clouds for each article were created, showing both their most frequently used words and their most frequently used words that carry a sentiment. Both word clouds are unsuprising for their respective articles, and the main difference between the two that the first article repeats its words more often, as indicated by a higher frequency of large font for its word cloud. The word coulds coded for sentiment for both articles confirm what was presented in the ggplots above. 

```{r echo = FALSE, message = FALSE,warning=FALSE}
###Wordcloud
custom_stop_words <- bind_rows(
  data_frame(
    word = c("plot"),
    lexicon = c("custom")
  ),
  stop_words
)
# look at the most common words in Article 1
Web1 %>%
  anti_join(custom_stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

# look at the most common words in Article 2
Web2 %>%
  anti_join(custom_stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
# find the most common positive and negative words in Article 1
Web1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(
    colors = c("red", "blue"),
    max.words = 100
  )
# find the most common positive and negative words in Article 2
Web2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(
    colors = c("red", "blue"),
    max.words = 100
  )
```


##Chapter3


The term frequency distribution plots the count of the number of words who's frequency of use increases as the graph moves to the right. This results in the peak towards the left of the graph indicating the number of words that are not used very often. The results for article 1 indicate that it has more words than article 2. It also shows that it uses a greater variety of words, as there are few data points as the frequency of use increases. However, based on the results of the previously produced graphs, it is suprising that there are a few more peaks, though admitidly small ones, towards the left of the x-axis. 

Zipf's law states that the frequency of a word's use is inversly proportioned to its rank. In other words, commonly used words that hold little meaning, such as "and" or "the" have a low rank, and scarcely used words have a higher rank. This would, in theory, cause a negative trend line. The two articles were graphed together, and though they overlap eachother fairly well, I would not say they are particularly comparable. The trendline is indeed negative, though the slope is not particularly steep. Both of these characteristics can be atributed to the shortness of the texts, and one being much more instructionally centered than the other. 

Term frequency-inverse document frequency (tf-idf), is used to measure how important a particular word is in a document out of a colection of documents, with more weight given to infrequently used words, and less weight given to commonly used words, implementing the theory of Zipf's law. Thus, the function will find words that are common to a text, but not so common that they have little significance. The results of the Cambridge Analytica article are not suprising, and are words that any article on the topic would contain. However, some of the results of the code run in the article are present in this analysis, causing sevral numbers to be included in the results. The resutls of the second article are also appropriate for the topic, and are largely words that describe statistical terms. Both results show words that are prominent in the word cloud. The results of the tf-idf analysis can again be seen with the highest scoring words in the final plot, though it should be noted that their scales are different before once compares them. 



```{r echo = FALSE, message=FALSE,warning=FALSE}
#Combine two articles into one dataset
Articles<- bind_rows(mutate(Web1, Webpage = "Article1"),
                       mutate(Web2, Webpage = "Article2")) %>% 
  count(Webpage, word)
#Count the total words for each article
total_words <- Articles %>% 
  group_by(Webpage) %>% 
  summarize(total = sum(n))

Articles <- left_join(Articles, total_words)
#plot the distribution of n/total for each article
ggplot(Articles, aes(n/total, fill = Webpage)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~Webpage, ncol = 2, scales = "free_y")+
  ggtitle("Term Frequency Distribution")+
  theme(plot.title = element_text(hjust = 0.5))

###Chapter 3.2

freq_by_rank <- Articles %>% 
  group_by(Webpage) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

freq_by_rank %>% 
  ggplot(aes(x = rank, y = `term frequency`, color = Webpage)) + 
  geom_abline(intercept = -2.65, slope = -0.07, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = TRUE) +   scale_x_log10() +
  scale_y_log10()+
    ggtitle("Zipf's Law")+
  theme(plot.title = element_text(hjust = 0.5))



####Chapter3.3
Articles <- Articles %>%
  bind_tf_idf(word, Webpage, n)
Articles %>%
  select(-total) %>%
  arrange(desc(tf_idf))
Articles %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(Webpage) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = Webpage)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~Webpage, ncol = 2, scales = "free") +
  coord_flip()

####Chapter 3.4
Articles <- Articles %>% bind_tf_idf(word, Webpage, n)
Articles %>% 
  select(-total) %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  top_n(10) %>% 
  group_by(Webpage) %>% 
  ungroup %>% 
  ggplot()+
  geom_col(aes(word,tf_idf,fill=Webpage))+
  labs(x=NULL,y="tf-idf")+
  facet_wrap(~Webpage,nrow=2,scales = "free")+
  coord_flip() 
```

##Chapter4


The tf-idf analysis was repeated for bigrams. Unfortunatly, due to the example code being printed in the first article and analyzed by the text reader, many of the bigrams are numbers, and do not actually have any significance. The results of the second article are as expected. 

The "Article 1 'not' Bigrams" shows the word most commonly following the word 'not'. This was repeated for article 2. The results are not particularly varied, due to the shortness of the texts. 

Bigram networks were created for each article, which indicate which commonly used words were followed by another commonly used word. Some words, such as "facebook" in the first article, were often followed by a variety of words, such as "data", "users", and "profiles". In this example, "facebook" was the center of the node, and the link connecting it to other words is the edge. In order to create a more informative graphic, arrows were added to show the direction the edge was moving, and the arrows were colored to show the weight of the bigram. The results for article 2 show that there are a few commonly used chains of words, and not many nodes. Article 1 has slightly more interesting results as there are more nodes with centers. However, there are again many numbers included, which are not meaningful.  

```{r echo = FALSE, message=FALSE,warning=FALSE}

Web_1 <- read.delim2("Article1.txt")
Web_2 <- read.delim2("Article2.txt")
colnames(Web_1) <- c("text")
colnames(Web_2) <- c("text")

Web1_bigrams <- Web_1 %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) # for p-value article
Web2_bigrams <- Web_2 %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) # for data-story article

bigrams_separated1 <- Web1_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered1 <- bigrams_separated1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts1 <- bigrams_filtered1 %>%
  count(word1, word2, sort = TRUE)

bigrams_separated2 <- Web2_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered2 <- bigrams_separated2 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts2 <- bigrams_filtered2 %>%
  count(word1, word2, sort = TRUE)


bigrams_united1 <- bigrams_filtered1 %>%
  unite(bigram, word1, word2, sep = " ")
bigrams_united2 <- bigrams_filtered2 %>%
  unite(bigram, word1, word2, sep = " ")
bigrams_united1 %>% mutate(Webpage = "Article1") -> bigrams_united1
bigrams_united2 %>% mutate(Webpage = "Article2") -> bigrams_united2


total_bigrams <- rbind(bigrams_united1, bigrams_united2)
total_bigrams %>%
  count(Webpage, bigram) %>%
  bind_tf_idf(bigram, Webpage, n) %>%
  arrange(desc(tf_idf)) %>%
  group_by(Webpage) %>%
  top_n(5) %>%
  ungroup() %>%
  ggplot() +
  geom_col(aes(bigram, tf_idf, fill = Webpage)) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~Webpage, nrow = 2, scales = "free") +
  coord_flip()

AFINN <- get_sentiments("afinn")

not_words1 <- bigrams_separated1 %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()
not_words2 <- bigrams_separated2 %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words1 %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip() +
  ggtitle("Article 1 'not' Bigrams")+
  theme(plot.title = element_text(hjust = 0.5))

not_words2 %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()+
  ggtitle("Article 2 'not' Bigrams")+
  theme(plot.title = element_text(hjust = 0.5))

bigram_counts1 <- bigrams_filtered1 %>%
  count(word1, word2, sort = TRUE)
bigram_counts2 <- bigrams_filtered2 %>%
  count(word1, word2, sort = TRUE)
# Wordcloud 1 for Article 1
bigram_graph1 <- bigram_counts1 %>%
  filter(n > 1) %>%
  graph_from_data_frame()
set.seed(2017)
ggraph(bigram_graph1, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  ggtitle("Article 1 Bigram Network")+
  theme(plot.title = element_text(hjust = 0.5))
# Wordcloud 2 for Article 1
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph1, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n),
    show.legend = FALSE,
    arrow = a, end_cap = circle(.07, "inches")
  ) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Article 1 Weighted Bigram Network")+
  theme(plot.title = element_text(hjust = 0.5))

# Wordcloud 1 for Article 2
bigram_graph2 <- bigram_counts2 %>%
  filter(n > 1) %>%
  graph_from_data_frame()
set.seed(2017)
ggraph(bigram_graph2, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  ggtitle("Article 2 Bigram Network")+
  theme(plot.title = element_text(hjust = 0.5))
# Wordcloud 2 for Article 2
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph2, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n),
    show.legend = FALSE,
    arrow = a, end_cap = circle(.07, "inches")
  ) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()+
  ggtitle("Article 2 Weighted Bigram Network")+
  theme(plot.title = element_text(hjust = 0.5))
```

## Conclusion
Text mining is a great way to systematically analyze and compare text documents in R. While there are limitations, such as lexicons not taking the different meanings of words into account, tidytext is still a versitile method of analysis. 

